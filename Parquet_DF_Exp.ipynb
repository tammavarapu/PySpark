{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parquet is a columnar format that is supported by many other data processing systems. \n",
    "Spark SQL provides support for both reading and writing Parquet files that automatically preserves the schema of the original data. \n",
    "When writing Parquet files, all columns are automatically converted to be nullable for compatibility reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import HiveContext,SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('paquet_df_exp').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonDF = spark.read.json('/dnbusr1/sambasivaraot/PySpark/input_data/people.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrames can be saved as Parquet files\n",
    "jsonDF.write.parquet('people.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the Parquet file \n",
    "prqt = spark.read.parquet('people.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prqt.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet files can also be used to create a temporary view and then used in SQL statements.\n",
    "prqt.createOrReplaceTempView('people_prqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "teenagers = spark.sql(\"SELECT * FROM people_prqt WHERE age >= 13 AND age <= 19\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|  name|\n",
      "+---+------+\n",
      "| 19|Justin|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teenagers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# spark is from the previous example.\n",
    "# Create a simple DataFrame, stored into a partition directory\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "squaresDF = spark.createDataFrame(sc.range(1,6).map(lambda i : Row(single=i,doublr=i ** 2)))\n",
    "squaresDF.write.parquet('data/test_table/key=1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubesDF = spark.createDataFrame(sc.range(6,11).map(lambda i : Row(single=i,triple=i ** 3)))\n",
    "cubesDF.write.parquet('data/test_table/key=2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+---+\n",
      "|doublr|single|triple|key|\n",
      "+------+------+------+---+\n",
      "|  null|     8|   512|  2|\n",
      "|  null|     7|   343|  2|\n",
      "|  null|     6|   216|  2|\n",
      "|  null|     9|   729|  2|\n",
      "|  null|    10|  1000|  2|\n",
      "|     1|     1|  null|  1|\n",
      "|     4|     2|  null|  1|\n",
      "|    16|     4|  null|  1|\n",
      "|     9|     3|  null|  1|\n",
      "|    25|     5|  null|  1|\n",
      "+------+------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mergeDF = spark.read.option('mergeSchema','true').parquet(\"data/test_table\")\n",
    "mergeDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata Refreshing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark is an existing SparkSession\n",
    "spark.catalog.refreshTable(\"my_table\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
