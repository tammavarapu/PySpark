{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"sampleApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg(*exprs)\n",
    "join(other, on=None, how=None)\n",
    "crossJoin(other)\n",
    "sort(*cols, **kwargs)\n",
    "sortWithinPartitions(*cols, **kwargs)\n",
    "groupBy(*cols)/groupby(*cols)\n",
    "orderBy(*cols, **kwargs)\n",
    "union(other)\n",
    "unionByName(other)\n",
    "hint(name, *parameters)\n",
    "coalesce(numPartitions)\n",
    "corr(col1, col2, method=None)\n",
    "randomSplit(weights, seed=None)\n",
    "repartition(numPartitions, *cols)\n",
    "repartitionByRange(numPartitions, *cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df\n",
    "df = spark.read.csv('/dnbusr1/sambasivaraot/PySpark/input_data/cruise_ship_info.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ship_name: string (nullable = true)\n",
      " |-- Cruise_line: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Tonnage: double (nullable = true)\n",
      " |-- passengers: double (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- cabins: double (nullable = true)\n",
      " |-- passenger_density: double (nullable = true)\n",
      " |-- crew: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+\n",
      "|  Ship_name|Cruise_line|Age|           Tonnage|passengers|length|cabins|passenger_density|crew|\n",
      "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+\n",
      "|    Journey|    Azamara|  6|30.276999999999997|      6.94|  5.94|  3.55|            42.64|3.55|\n",
      "|      Quest|    Azamara|  6|30.276999999999997|      6.94|  5.94|  3.55|            42.64|3.55|\n",
      "|Celebration|   Carnival| 26|            47.262|     14.86|  7.22|  7.43|             31.8| 6.7|\n",
      "|   Conquest|   Carnival| 11|             110.0|     29.74|  9.53| 14.88|            36.99|19.1|\n",
      "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### agg(*exprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|max(Age)|\n",
      "+--------+\n",
      "|      48|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({\"Age\":\"max\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|max(Tonnage)|\n",
      "+------------+\n",
      "|       220.0|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(F.max(df.Tonnage)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join(other, on=None, how=None)\n",
    "###### other - Right side of the join\n",
    "###### on    - \n",
    "###### how   â€“ str, default inner. Must be one of: inner, cross, outer, full, full_outer, left, left_outer, right, right_outer, left_semi, and left_anti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a string for the join column name\n",
    "df.join(df2, df.name == df2.name, 'outer').select(df.name,df2.height).show()\n",
    "df.join(df2, 'name', 'outer').select('name','height').show()\n",
    "\n",
    "# a list of Columns\n",
    "cond = [df.name == df3.name, df.age == df3.age]\n",
    "df.join(df2, cond, 'outer').select(df.name,df2.height).show()\n",
    "\n",
    "df.join(df2,'name').select('name','height').show()\n",
    "df.join(df2,['name','age']).select(df.name, df.age).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data\n",
    "valuesA = [(1, 'bob', 3462543658686),(2, 'rob', 9087567565439),(3, 'tim', 5436586999467),(4, 'tom', 8349756853250)]\n",
    "valuesB = [(1, 'ketchup', 'bob', 1.20),(2, 'rutabaga', 'bob', 3.35),(3, 'fake vegan meat', 'rob', 13.99),(4, 'cheesey poofs', 'tim', 3.99),(5, 'ice cream', 'tim', 4.95),(6, 'protein powder', 'tom', 49.95)]\n",
    "\n",
    "# create DF\n",
    "customerDF = spark.createDataFrame(valuesA,['id','name','credit_card_number'])\n",
    "ordersDF = spark.createDataFrame(valuesB,['id', 'product_name', 'customer', 'price'])\n",
    "\n",
    "# inner_join(default)\n",
    "joinedDF = customerDF.join( ordersDF, customerDF.name == ordersDF.customer )\n",
    "joinedDF = customerDF.join( ordersDF, customerDF.name == ordersDF.customer, 'inner')\n",
    "joinedDF.show()\n",
    "\n",
    "# right_join, \n",
    "right_joinDF = customerDF.join( ordersDF, customerDF.name == ordersDF.customer, 'right')\n",
    "\n",
    "# left_join\n",
    "left_joinDF = customerDF.join( ordersDF, customerDF.name == ordersDF.customer, 'left')\n",
    "\n",
    "# cross_join\n",
    "joinedDF = customerDF.crossJoin(ordersDF)\n",
    "\n",
    "## Aggregating_Data\n",
    "Two functions here: agg() and groupBy(). \n",
    "These are typically used in tandem, but agg() can be used on a dataset without groupBy()\n",
    "\n",
    "#df.agg({\"<df column>\":\"function\"}).show()\n",
    "\n",
    "df.agg({\"Sales\":\"min\"}).show()\n",
    "\n",
    "from pyspark.sql import function as F\n",
    "\n",
    "df.groupBy('borough').agg(F.count('borough').alias('count')).show()\n",
    "\n",
    "df.groupBy('borough').agg(F.sum('number_of_persons_injured').alias('injuries')).orderBy('injuries', ascending=False).show()\n",
    "\n",
    "## Grouping_by_multiple_columns\n",
    "aggDF = df.groupBy('borough','contributing_factor_vehicle_1').agg(F.sum('number_of_persons_injured').alias('injuries')).orderBy('injuries', ascending=False).show()\n",
    "\n",
    "Note: So far we've aggregated by using the count and sum functions. As you might imagine, we could also aggregate by using the min, max, and avg functions. \n",
    "\n",
    "## Determining Column Correlation\n",
    "corr() determines whether two columns have any correlation between them, and outputs and integer which represent the correlation:\n",
    "df.agg(corr(\"a\", \"b\").alias('correlation')).collect()\n",
    "\n",
    "## Import CSV File into Spark Dataframe\n",
    "import pyspark as spark\n",
    " \n",
    "sc = spark.SQLContext(spark.SparkContext()) \n",
    "sdf1 = sc.read.csv(\"Documents/nycflights13.csv\", header = True, inferSchema = True)\n",
    "\n",
    "# Data Aggregation with Spark Dataframe\n",
    "import pyspark.sql.functions as fn\n",
    " \n",
    "sdf1.cache().filter(\"month in (1, 3, 5)\").groupby(\"month\").agg(fn.mean(\"dep_time\").alias(\"avg_dep\"), fn.mean(\"arr_time\").alias(\"avg_arr\")).show()\n",
    "\n",
    "# Data Aggregation with Spark SQL\n",
    "sc.registerDataFrameAsTable(sdf1, \"tbl1\")\n",
    " \n",
    "sc.sql(\"select month, avg(dep_time) as avg_dep, avg(arr_time) as avg_arr from tbl1 where month in (1, 3, 5) group by month\").show()\n",
    " \n",
    "sc.dropTempTable(sc.tableNames()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### crossJoin(other)\n",
    "#### Returns the cartesian product with another DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"age\", \"name\").collect()\n",
    "df2.select(\"name\", \"height\").collect()\n",
    "\n",
    "df.crossJoin(df2.select(\"height\")).select(\"age\", \"name\", \"height\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sort(*cols, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a new DataFrame sorted by the specified column(s).\n",
    "df.sort(df.age.desc()).collect()\n",
    "df.sort(\"age\", ascending=False).collect()\n",
    "\n",
    "df.orderBy(df.age.desc()).collect()\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "df.sort(asc(\"age\")).collect()\n",
    "df.orderBy(desc(\"age\"), \"name\").collect()\n",
    "\n",
    "df.orderBy([\"age\", \"name\"], ascending=[0, 1]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sortWithinPartitions(*cols, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a new DataFrame with each partition sorted by the specified column(s).\n",
    "df.sortWithinPartitions(\"age\", ascending=False).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
