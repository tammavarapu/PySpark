{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the default data source (parquet unless otherwise configured by spark.sql.sources.default) \n",
    "df = spark.read.load(\"examples/src/main/resources/users.parquet\")\n",
    "df.select(\"name\", \"favorite_color\").write.save(\"namesAndFavColors.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually Specifying Options\n",
    "df = spark.read.load(\"examples/src/main/resources/people.json\", format=\"json\")\n",
    "df.select(\"name\", \"age\").write.save(\"namesAndAges.parquet\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load a CSV file you can use:\n",
    "df = spark.read.load(\"examples/src/main/resources/people.csv\",format=\"csv\", sep=\":\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The extra options are also used during write operation. \n",
    "# For example, you can control bloom filters and dictionary encodings for ORC data sources. \n",
    "# The following ORC example will create bloom filter on favorite_color and use dictionary encoding for name and favorite_color. \n",
    "# For Parquet, there exists parquet.enable.dictionary, too.\n",
    "\n",
    "# read df\n",
    "df = spark.read.orc(\"examples/src/main/resources/users.orc\")\n",
    "\n",
    "# write df\n",
    "df.write.format(\"orc\")\n",
    "    .option(\"orc.bloom.filter.columns\", \"favorite_color\")\n",
    "    .option(\"orc.dictionary.key.threshold\", \"1.0\")\n",
    "    .save(\"users_with_options.orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run_SQL_on_files_directly\n",
    "df = spark.sql(\"SELECT * FROM parquet.`examples/src/main/resources/users.parquet`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Modes\n",
    "\"error\" or \"errorifexists\" (default) - if data already exists, an exception is expected to be thrown.\n",
    "\"append\"    - if data/table already exists, contents of the DataFrame are expected to be appended to existing data.\n",
    "\"overwrite\" - if data/table already exists, existing data is expected to be overwritten by the contents of the DataFrame.\n",
    "\"ignore\"    - if data already exists, the save operation is expected not to save the contents of the DataFrame and not to change the existing data.\n",
    "             This is similar to a CREATE TABLE IF NOT EXISTS in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving to Persistent Tables\n",
    "\n",
    "\n",
    "\n",
    "For file-based data source, e.g. text, parquet, json, etc. you can specify a custom table path via the path option, \n",
    "e.g. df.write.option(\"path\", \"/some/path\").saveAsTable(\"t\").\n",
    "When the table is dropped, the custom table path will not be removed and the table data is still there. \n",
    "If no custom table path is specified, Spark will write data to a default table path under the warehouse directory. \n",
    "When the table is dropped, the default table path will be removed too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucketing, Sorting and Partitioning\n",
    "\n",
    "# For file-based data source, it is also possible to bucket and sort or partition the output. \n",
    "  Bucketing and sorting are applicable only to persistent tables:\n",
    "    \n",
    "df.write.bucketBy(42, \"name\").sortBy(\"age\").saveAsTable(\"people_bucketed\")\n",
    "\n",
    "# while partitioning can be used with both save and saveAsTable when using the Dataset APIs.\n",
    "\n",
    "df.write.partitionBy(\"favorite_color\").format(\"parquet\").save(\"namesPartByColor.parquet\")\n",
    "\n",
    "# It is possible to use both partitioning and bucketing for a single table:\n",
    "\n",
    "df = spark.read.parquet(\"examples/src/main/resources/users.parquet\")\n",
    "\n",
    "df.write\n",
    "    .partitionBy(\"favorite_color\")\n",
    "    .bucketBy(42, \"name\")\n",
    "    .saveAsTable(\"people_partitioned_bucketed\")\n",
    "    \n",
    "\n",
    "partitionBy creates a directory structure as described in the Partition Discovery section. \n",
    "Thus, it has limited applicability to columns with high cardinality. \n",
    "In contrast bucketBy distributes data across a fixed number of buckets and \n",
    "can be used when a number of unique values is unbounded.    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
