{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/cloudera/parcels/SPARK2/lib/spark2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['JAVA_HOME'] = \"/usr/java/jdk1.8.0_151-cloudera\"\n",
    "#os.environ['PYSPARK_PYTHON'] = \"/sasgridL2/PythonApp/anaconda3/bin/python\"\n",
    "os.environ['JAVA_HOME'] = \"/usr/java/jdk1.8.0_151-cloudera\"\n",
    "os.environ['PYSPARK_PYTHON'] = \"/opt/anaconda3/bin/python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf \n",
    "from pyspark.sql import HiveContext, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creates a DataFrame from an RDD, a list or a pandas.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.createDataFrame(data, schema=None) \n",
    "data – an RDD of any kind of SQL data representation(e.g. row, tuple, int, boolean, etc.), \n",
    "    or list, or pandas.DataFrame.(Creates a DataFrame from an RDD, a list or a pandas.DataFrame.)\n",
    "schema – It will be inferred from data.When schema is None, it will try to infer the schema (column names and types) from data,\n",
    "     which should be an RDD of Row, or namedtuple, or dict.\n",
    "\n",
    "--list\n",
    "l = [('Alice', 1)]\n",
    "df = spark.createDataFrame(l).show()\n",
    "df = spark.createDataFrame(l,['name','age']).show()\n",
    "df = spark.createDataFrame([(\"10\", ), (\"11\", ), (\"13\",  )], [\"age\"])\n",
    "df = spark.createDataFrame([\"10\",\"11\",\"13\"], \"string\").toDF(\"age\")\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "df = spark.createDataFrame([\"10\",\"11\",\"13\"], StringType()).toDF(\"age\")\n",
    "\n",
    "\n",
    "from pyspark.sql import Row\n",
    "l = [('Ankit',25),('Jalfaizy',22),('saurabh',20),('Bala',26)]\n",
    "rdd = sc.parallelize(l)\n",
    "people = rdd.map(lambda x: Row(name=x[0], age=int(x[1])))\n",
    "schemaPeople = sqlContext.createDataFrame(people)\n",
    "\n",
    "df = sqlContext.createDataFrame([(1.0, 0.3, 1.0), (1.0, 0.5, 0.0), (-1.0, 0.6, 0.5), (-1.0, 5.6, 0.2)], (\"col1\", \"col2\", \"col3\"))\n",
    "df.show()\n",
    "\n",
    "--dict\n",
    "d = [{'name': 'Alice', 'age': 1}]\n",
    "df = spark.createDataFrame(d).show()\n",
    "df = spark.createDataFrame([{\"a\": \"x\", \"b\": \"y\", \"c\": \"3\"}])\n",
    "\n",
    "--rdd_scenario-1:\n",
    "l = [('Alice', 1)]\n",
    "rdd = sc.parallelize(l)\n",
    "df = spark.createDataFrame(rdd).collect()\n",
    "df = spark.createDataFrame(rdd, ['name', 'age'])\n",
    "df = spark.createDataFrame(rdd, \"name: string, age: int\").collect()\n",
    "\n",
    "df = sc.parallelize([[1,2,3], [2,3,4]]).toDF()\n",
    "df = sc.parallelize([[1,2,3], [2,3,4]]).toDF((\"a\", \"b\", \"c\"))\n",
    "df = sc.parallelize([[25, 'Prem', 'M', '12-21-2006 11:00:05','abc', '1'],\n",
    "                     [20, 'Kate', 'F', '05-30-2007 10:05:00', 'asdf', '2'],\n",
    "                     [40, 'Cheng', 'M', '12-30-2017 01:00:01', 'qwerty', '3']])\\\n",
    "    .toDF([\"age\",\"name\",\"sex\",\"datetime_in_strFormat\",\"initial_col_name\",\"col_in_strFormat\"])\n",
    "\n",
    "--rdd_scenario-2:\n",
    "from pyspark.sql import Row\n",
    "Person = Row('name', 'age')\n",
    "l = [('Alice', 1)]\n",
    "rdd = sc.parallelize([('Alice', 1)])\n",
    "person = rd.map(lambda r: Person(*r)) # person = rd.map(lambda r: Row('name'=r[0],'age'=r[1])\n",
    "df2 = spark.createDataFrame(person)\n",
    "df2.collect()\n",
    "\n",
    "--rdd_scenario-3:\n",
    "from pyspark.sql.types import *\n",
    "schema = StructType([StructField(\"name\", StringType(), True),StructField(\"age\", IntegerType(), True)])\n",
    "rdd = sc.parallelize([('Alice', 1)])\n",
    "df3 = spark.createDataFrame(rdd, schema)\n",
    "df3.collect()\n",
    "\n",
    "--pandas\n",
    "spark.createDataFrame(df.toPandas()).collect()\n",
    "\n",
    "spark.createDataFrame(pd.DataFrame([[1,2]])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
