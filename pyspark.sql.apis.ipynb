{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spark sql modules\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark instance with app name \"sampleApp\"\n",
    "spark = SparkSession.builder.appName(\"sampleApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark.sql.SparkSession\n",
    "pyspark.sql.DataFrame\n",
    "pyspark.sql.Column\n",
    "pyspark.sql.Row\n",
    "pyspark.sql.GroupedData\n",
    "pyspark.sql.DataFrameNaFunctions\n",
    "pyspark.sql.DataFrameStatFunctions\n",
    "pyspark.sql.functions\n",
    "pyspark.sql.types\n",
    "pyspark.sql.Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appName(name)\n",
    "config(key=None, value=None, conf=None)\n",
    "enableHiveSupport()\n",
    "getOrCreate()\n",
    "master(master)\n",
    "createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True)\n",
    "newSession()\n",
    "range(start, end=None, step=1, numPartitions=None)\n",
    "sql(sqlQuery)\n",
    "stop()\n",
    "table(tableName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSession Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf\n",
    "builder\n",
    "sparkContext\n",
    "\n",
    "catalog\n",
    "\n",
    "read\n",
    "readStream\n",
    "streams\n",
    "\n",
    "udf\n",
    "\n",
    "version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark.sql.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create df\n",
    "toDF(*cols)\n",
    "toJSON(use_unicode=True)\n",
    "toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reg as temptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createGlobalTempView(name)\n",
    "createOrReplaceGlobalTempView(name)\n",
    "createOrReplaceTempView(name)\n",
    "createTempView(name)\n",
    "registerTempTable(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### working on top of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select(*cols)\n",
    "selectExpr(*expr)\n",
    "alias(alias)\n",
    "colRegex(colName)\n",
    "filter(condition)/where(condition)\n",
    "cube(*cols)\n",
    "rollup(*cols)\n",
    "drop(*cols)\n",
    "dropDuplicates(subset=None)/drop_duplicates(subset=None)\n",
    "dropna(how='any', thresh=None, subset=None)\n",
    "fillna(value, subset=None)\n",
    "withColumn(colName, col)\n",
    "withColumnRenamed(existing, new)\n",
    "replace(to_replace, value=<no value>, subset=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perform on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg(*exprs)\n",
    "crossJoin(other)\n",
    "join(other, on=None, how=None)\n",
    "sort(*cols, **kwargs)\n",
    "sortWithinPartitions(*cols, **kwargs)\n",
    "groupBy(*cols)/groupby(*cols)\n",
    "orderBy(*cols, **kwargs)\n",
    "union(other)\n",
    "unionByName(other)\n",
    "hint(name, *parameters)\n",
    "coalesce(numPartitions)\n",
    "corr(col1, col2, method=None)\n",
    "randomSplit(weights, seed=None)\n",
    "repartition(numPartitions, *cols)\n",
    "repartitionByRange(numPartitions, *cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run operatitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersect(other)\n",
    "intersectAll(other)\n",
    "subtract(other)\n",
    "count()\n",
    "distinct()\n",
    "crosstab(col1, col2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statical Summary of Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(*cols)\n",
    "cov(col1, col2)\n",
    "sample(withReplacement=None, fraction=None, seed=None)\n",
    "sampleBy(col, fractions, seed=None)\n",
    "summary(*statistics)\n",
    "approxQuantile(col, probabilities, relativeError)\n",
    "freqItems(cols, support=None)\n",
    "exceptAll(other)\n",
    "explain(extended=False)\n",
    "withWatermark(eventTime, delayThreshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect()\n",
    "first()\n",
    "foreach(f)\n",
    "foreachPartition(f)\n",
    "head(n=None)\n",
    "limit(num)\n",
    "take(num)\n",
    "show(n=20, truncate=True, vertical=False)\n",
    "printSchema()\n",
    "toLocalIterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isLocal()\n",
    "cache()\n",
    "persist(storageLevel=StorageLevel(True, True, False, False, 1))\n",
    "unpersist(blocking=False)\n",
    "checkpoint(eager=True)\n",
    "localCheckpoint(eager=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DF Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema\n",
    "columns\n",
    "dtypes\n",
    "\n",
    "rdd\n",
    "\n",
    "na\n",
    "stat\n",
    "\n",
    "write\n",
    "\n",
    "writeStream\n",
    "isStreaming\n",
    "\n",
    "storageLevel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg(*exprs)\n",
    "apply(udf)\n",
    "avg(*cols)\n",
    "count()\n",
    "max(*cols)\n",
    "mean(*cols)\n",
    "min(*cols)\n",
    "pivot(pivot_col, values=None)\n",
    "sum(*cols)[source]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark.sql.Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astype(dataType)\n",
    "cast(dataType)\n",
    "\n",
    "name(*alias, **kwargs)\n",
    "contains(other)\n",
    "substr(startPos, length)\n",
    "between(lowerBound, upperBound)\n",
    "like(other)\n",
    "rlike(other)\n",
    "otherwise(value)\n",
    "over(window)\n",
    "when(condition, value)\n",
    "getField(name)\n",
    "getItem(key)\n",
    "\n",
    "bitwiseAND(other)\n",
    "bitwiseOR(other)\n",
    "bitwiseXOR(other)\n",
    "\n",
    "desc()\n",
    "desc_nulls_first()\n",
    "desc_nulls_last()\n",
    "\n",
    "asc()\n",
    "asc_nulls_first()\n",
    "asc_nulls_last()\n",
    "\n",
    "endswith(other)\n",
    "startswith(other)\n",
    "\n",
    "eqNullSafe(other)\n",
    "isNotNull()\n",
    "isNull()\n",
    "isin(*cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark.sql.Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = Row(name=\"Alice\", age=11)\n",
    "asDict(recursive=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  pyspark.sql.DataFrameNaFunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop(how='any', thresh=None, subset=None)\n",
    "fill(value, subset=None)\n",
    "replace(to_replace, value=<no value>, subset=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark.sql.DataFrameStatFunctions(df)[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approxQuantile(col, probabilities, relativeError)\n",
    "corr(col1, col2, method=None)\n",
    "cov(col1, col2)\n",
    "crosstab(col1, col2)\n",
    "freqItems(cols, support=None)\n",
    "sampleBy(col, fractions, seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark.sql.DataFrameWriter(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucketBy(numBuckets, col, *cols)\n",
    "csv(path, mode=None, compression=None, sep=None, quote=None, escape=None, header=None, nullValue=None, escapeQuotes=None, quoteAll=None, dateFormat=None, timestampFormat=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, charToEscapeQuoteEscaping=None, encoding=None, emptyValue=None)\n",
    "format(source)\n",
    "insertInto(tableName, overwrite=False)\n",
    "jdbc(url, table, mode=None, properties=None)\n",
    "json(path, mode=None, compression=None, dateFormat=None, timestampFormat=None, lineSep=None, encoding=None)\n",
    "mode(saveMode)\n",
    "option(key, value)\n",
    "options(**options)\n",
    "orc(path, mode=None, partitionBy=None, compression=None)\n",
    "parquet(path, mode=None, partitionBy=None, compression=None)\n",
    "partitionBy(*cols)\n",
    "save(path=None, format=None, mode=None, partitionBy=None, **options)\n",
    "saveAsTable(name, format=None, mode=None, partitionBy=None, **options)\n",
    "sortBy(col, *cols)\n",
    "text(path, compression=None, lineSep=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark.sql.DataFrameReader(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv(path, schema=None, sep=None, encoding=None, quote=None, escape=None, comment=None, header=None, inferSchema=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, nullValue=None, nanValue=None, positiveInf=None, negativeInf=None, dateFormat=None, timestampFormat=None, maxColumns=None, maxCharsPerColumn=None, maxMalformedLogPerPartition=None, mode=None, columnNameOfCorruptRecord=None, multiLine=None, charToEscapeQuoteEscaping=None, samplingRatio=None, enforceSchema=None, emptyValue=None)\n",
    "format(source)\n",
    "jdbc(url, table, column=None, lowerBound=None, upperBound=None, numPartitions=None, predicates=None, properties=None)\n",
    "json(path, schema=None, primitivesAsString=None, prefersDecimal=None, allowComments=None, allowUnquotedFieldNames=None, allowSingleQuotes=None, allowNumericLeadingZero=None, allowBackslashEscapingAnyCharacter=None, mode=None, columnNameOfCorruptRecord=None, dateFormat=None, timestampFormat=None, multiLine=None, allowUnquotedControlChars=None, lineSep=None, samplingRatio=None, dropFieldIfAllNull=None, encoding=None)\n",
    "load(path=None, format=None, schema=None, **options)\n",
    "option(key, value)\n",
    "options(**options)\n",
    "orc(path)\n",
    "parquet(*paths)\n",
    "schema(schema)\n",
    "table(tableName)\n",
    "text(paths, wholetext=False, lineSep=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
